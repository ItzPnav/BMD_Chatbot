
# ğŸ“„ **FULL SETUP of EMBEDDINGS  â€” Embeddings Microservice Setup (bge-m3)**

# ğŸ§¬ **BGE-M3 Embeddings Service Setup Guide**

*(Used by BMD_Chatbot for semantic search and RAG retrieval)*

This guide explains how to install, configure, and run the **bge-m3 embeddings microservice** locally using Docker and HuggingFaceâ€™s `text-embeddings-inference` server.

This service generates **1024-dimensional dense embeddings**, which your backend stores in **pgvector** and uses for similarity search.

---

# ğŸŒ Official Reference Links (Required Reading)

### ğŸ”— HuggingFace bge-m3 model

[https://huggingface.co/BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)

### ğŸ”— Text Embeddings Inference Server (Docker Image)

[https://huggingface.co/docs/text-embeddings-inference/index](https://huggingface.co/docs/text-embeddings-inference/index)

### ğŸ”— Model Architecture Explanation

[https://huggingface.co/blog/bge-m3](https://huggingface.co/blog/bge-m3)

---

# ğŸ“ **Folder Structure**

Place your embeddings model inside:

```
BMD_Chatbot/
â””â”€â”€ infra/
    â””â”€â”€ embeddings/
        â””â”€â”€ models/
            â””â”€â”€ bge-m3/
                â”œâ”€â”€ config.json
                â”œâ”€â”€ model.safetensors
                â”œâ”€â”€ tokenizer.json
                â”œâ”€â”€ special_tokens_map.json
                â””â”€â”€ other model files...
```

âš ï¸ **The folder must contain the full HuggingFace model**.

If missing files â†’ the embeddings server will fail to boot.

---

# ğŸ§© **About bge-m3 (Why This Model?)**

bge-m3 is a **multi-function embedding model**:

* ğŸ”¹ Dense Embeddings (Semantic search)
* ğŸ”¹ Sparse Embeddings (BM25-like signals)
* ğŸ”¹ Multi-vector Embeddings
* ğŸ”¹ 1024-dim embedding size
* ğŸ”¹ Excellent for retrieval-augmented generation (RAG)

It gives **state-of-the-art recall** across languages.

---

# ğŸ³ **Running Embeddings Service with Docker**

The embeddings service runs using HuggingFace's official container:

```
ghcr.io/huggingface/text-embeddings-inference:cpu-1.4
```

Read more:
[https://github.com/huggingface/text-embeddings-inference](https://github.com/huggingface/text-embeddings-inference)

---

# âš™ï¸ **Embeddings Service Configuration (docker-compose.yml)**

Your `infra/docker-compose.yml` should include:

```yaml
embeddings:
  image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.4
  container_name: bmd_embeddings
  restart: always
  ports:
    - "8088:80"
  volumes:
    - ./embeddings/models/bge-m3:/data
  environment:
    MODEL_ID: /data
    NUM_THREADS: 4
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:80/health"]
    interval: 10s
    retries: 5
```

---

# ğŸš€ **Starting the Embeddings Service**

From inside:

```
BMD_Chatbot/infra
```

Run:

```bash
docker compose up --build
```

Watch logs:

```bash
docker logs -f bmd_embeddings
```

You should see:

```
Model loaded from /data
Running on port 80
```

---

# ğŸŒ **Embeddings API Documentation**

Official docs:
[https://huggingface.co/docs/text-embeddings-inference/quickstart](https://huggingface.co/docs/text-embeddings-inference/quickstart)

### ğŸ’¡ Your local embeddings API runs at:

```
http://localhost:8088
```

### Main endpoint:

```
POST /embed
```

---

# ğŸ§ª **Test the Embeddings API**

Use curl:

```bash
curl -X POST http://localhost:8088/embed   -H "Content-Type: application/json"   -d '{"input": ["hello world"]}'
```

Expected response:

```json
{
  "embedding": [[0.0123, -0.0044, ... 1024 dims ...]]
}
```

---

# ğŸ”„ **How Your Backend Uses This Service**

Inside `backend/src/services/embeddingService.js`, the backend sends:

```js
POST http://localhost:8088/embed
```

It receives the embedding vector, converts it to pgvector format, and stores it.

---

# ğŸ§± **Embedding Dimensions**

bge-m3 outputs **1024-dimensional vectors**.

Your table must use:

```sql
embedding vector(1024)
```

If you use a different model â†’ update dimension.

---

# âš ï¸ **Common Errors & Fixes**

---

## âŒ `Model file not found: config.json`

Fix: Ensure model path is correct inside:

```
infra/embeddings/models/bge-m3/
```

---

## âŒ `embedding_service timeout` in backend

Fix: Service may be loading slowly.

Increase timeout or restart:

```bash
docker restart bmd_embeddings
```

---

## âŒ `vector dimensions mismatch`

Check:

```sql
SELECT dimensions(embedding) FROM embeddings LIMIT 1;
```

Must equal **1024**.

---

## âŒ `HTTP 500` from embeddings API

Likely because:

* model didn't load
* folder empty
* corrupted safetensors file
* docker volume mount incorrect

Check logs:

```bash
docker logs bmd_embeddings
```

---

# ğŸ“¦ **Production Deployment Recommendations**

âœ” Use GPU version of container for high throughput
âœ” Increase NUM_THREADS for CPU serving
âœ” Move model folder to mounted SSD
âœ” Restart service nightly for memory cleanup
âœ” Enable container auto-restart (done already)
