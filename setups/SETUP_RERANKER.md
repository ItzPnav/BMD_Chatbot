
# ğŸ“„ **SETUP_RERANKER.md â€” Reranker Microservice (bge-reranker) Setup Guide**


# ğŸ¯ **Overview**

The BMD_Chatbot uses a **second-stage re-ranking model** to refine semantic search results.
After embeddings retrieval (via bge-m3), this microservice applies a **cross-encoder model** to:

* Improve ranking accuracy
* Boost final answer relevance
* Ensure top-K context is best possible for Claude

Your reranker model:

```
bge-reranker
(local HuggingFace folder)
```

---

# ğŸŒ **Official Links**

### ğŸ”— HuggingFace Original Model (BAAI/bge-reranker-base)

[https://huggingface.co/BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)

### ğŸ”— HuggingFace Text-Inference Server (Reranker mode)

[https://huggingface.co/docs/text-embeddings-inference/rerank](https://huggingface.co/docs/text-embeddings-inference/rerank)

### ğŸ”— Docker Image

[https://github.com/huggingface/text-embeddings-inference](https://github.com/huggingface/text-embeddings-inference)

---

# ğŸ“ **Folder Structure**

Place your reranker model at:

```
BMD_Chatbot/
â””â”€â”€ infra/
    â””â”€â”€ reranker/
        â””â”€â”€ models/
            â””â”€â”€ bge-reranker/
                â”œâ”€â”€ config.json
                â”œâ”€â”€ model.safetensors
                â”œâ”€â”€ tokenizer.json
                â”œâ”€â”€ special_tokens_map.json
                â””â”€â”€ other model files...
```

âš ï¸ These files **must exist**.
If any is missing â†’ model server will fail to start.

---

# ğŸ§© **How the Reranker Works in the RAG Pipeline**

```
User Query
    â”‚
    â”œâ”€â”€ Step 1: Embedding search (bge-m3 â†’ Postgres)
    â”‚
    â””â”€â”€ Step 2: Reranker compares:
          - Query text
          - Each retrieved chunk
          â†’ Produces a relevance score
```

Reranker output shape:

```json
[
  {
    "text": "document chunk...",
    "score": 0.874,
    "metadata": {...}
  }
]
```

Backend takes the top-K (default: 3) â†’ sends to Claude.

---

# ğŸ³ **Docker Setup for the Reranker Service**

In your `infra/docker-compose.yml` you must include:

```yaml
reranker:
  image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.4
  container_name: bmd_reranker
  restart: always
  ports:
    - "8091:80"
  volumes:
    - ./reranker/models/bge-reranker:/data
  environment:
    MODEL_ID: /data
    TASK: rerank
    NUM_THREADS: 4
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:80/health"]
    interval: 10s
    retries: 5
```

---

# ğŸš€ **Start the Reranker**

From:

```
cd BMD_Chatbot/infra
```

Run:

```bash
docker compose up --build
```

Check logs:

```bash
docker logs -f bmd_reranker
```

Expected:

```
loading rerank model...
running on port 80...
```

---

# ğŸŒ **API Endpoint Reference**

Local reranker runs at:

```
http://localhost:8091
```

### Primary endpoint:

```
POST /rerank
```

### Official API Docs:

[https://huggingface.co/docs/text-embeddings-inference/rerank](https://huggingface.co/docs/text-embeddings-inference/rerank)

---

# ğŸ§ª **Test the Reranker Service**

Use curl:

```bash
curl -X POST http://localhost:8091/rerank   -H "Content-Type: application/json"   -d '{"query":"best temple","documents":["Temple A info","Temple B info"]}'
```

Expected output:

```json
[
  {"text": "Temple B info", "score": 0.93},
  {"text": "Temple A info", "score": 0.78}
]
```

---

# ğŸ”„ **How Backend Uses the Reranker**

Your backend calls:

```js
const reranked = await rerankerService.rerank(query, documents)
```

Service request includes:

* query
* array of chunks
* top_k parameter

Used inside:

```
backend/src/controllers/chatController.js
```

This reranked output determines **final context quality** for Claude.

---

# ğŸ§± **Best Practices for Reranker Performance**

### âœ” Use SSD storage for model folder

### âœ” Increase NUM_THREADS if CPU is strong

### âœ” Warm up server after start by sending dummy request

### âœ” Use GPU container for high-load API servers

### âœ” Expose health endpoint via API gateway

---

# âš ï¸ **Common Issues & Fixes**

---

### âŒ `Could not load model from /data`

Cause:

* Wrong volume mount
* Missing files
* Folder not readable

Fix:

* Ensure folder exists:
  `infra/reranker/models/bge-reranker/*`

---

### âŒ HTTP 500 errors from `/rerank`

Cause:

* Model not fully loaded before request
  Fix:
* Add healthcheck (already configured)
* Retry request after a few seconds

---

### âŒ Slow reranking

Reranking is heavier than embedding search.
Fixes:

* Increase CPU threads
* Use GPU
* Reduce context size before rerank step

---

# ğŸ§  **Production Tuning Tips**

âœ” For high traffic: deploy multiple reranker replicas
âœ” Use Nginx load balancing in front
âœ” Cache reranker responses by query
âœ” Pre-rerank cached documents if your dataset rarely changes

---

# ğŸ‰ **Reranker Microservice Ready!**

Your **bge-reranker** service is now fully operational and integrated into the BMD RAG pipeline.

You can now enjoy:

* Cleaner context
* Higher accuracy
* Better answers
* Smarter semantic matching


